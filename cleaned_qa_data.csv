Question,Answer,Difficulty
What is linear regression? When do we use it?,Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).,easy
What’s the normal distribution? Why do we care about it?,The normal distribution is a continuous probability distribution whose probability density function takes the following formula:,easy
How do we check if a variable follows the normal distribution? ‍,"1. Plot a histogram out of the sampled data. If you can fit the bell-shaped normal curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.",medium
What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? ‍,"Data is not normal. Specially, real-world datasets or uncleaned datasets always have certain skewness. Same goes for the price prediction. Price of houses or any other thing under consideration depends on a number of factors. So, theres a great chance of presence of some skewed values i.e outliers if we talk in data science terms.",medium
What methods for solving linear regression do you know? ‍,"To solve linear regression, you need to find the coefficients which minimize the sum of squared errors.",medium
What is gradient descent? How does it work? ‍,"Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.",medium
What is the normal equation? ‍,Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.,medium
What is SGD — stochastic gradient descent? What’s the difference with the usual gradient descent? ‍,"In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.",medium
Which metrics for evaluating regression models do you know?,1. Mean Squared Error(MSE),easy
What are MSE and RMSE?,MSE stands for Mean Square Error while RMSE stands for Root Mean Square Error. They are metrics with which we can evaluate models.,easy
What is the bias-variance trade-off?,"**Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.",easy
How to validate your models?,"One of the most common approaches is splitting data into train, validation and test parts.",easy
"Why do we need to split our data into three parts: train, validation, and test?","The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never seen before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.",easy
Can you explain how cross-validation works?,"Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting different training and validation set, in order to reduce the bias that you would have by selecting only one validation set.",easy
What is K-fold cross-validation?,"K fold cross validation is a method of cross validation where we select a hyperparameter k. The dataset is now divided into k parts. Now, we take the 1st part as validation set and remaining k-1 as training set. Then we take the 2nd part as validation set and remaining k-1 parts as training set. Like this, each part is used as validation set once and the remaining k-1 parts are taken together and used as training set.",easy
How do we choose K in K-fold cross-validation? What’s your favorite K?,"There are two things to consider while deciding K: the number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained. ",easy
What is logistic regression? When do we need to use it?,"Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, spam and not spam, churn and not churn and so on. The variable is said to be a binary or dichotomous.",easy
Is logistic regression a linear model? Why?,"Yes, Logistic Regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters.",easy
What is sigmoid? What does it do?,"A sigmoid function is a type of activation function, and more specifically defined as a squashing function. Squashing functions limit the output to a range between 0 and 1, making these functions useful in the prediction of probabilities.",easy
How do we evaluate classification models?,"Depending on the classification problem, we can use the following evaluation metrics:",easy
What is accuracy?,Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.,easy
Is accuracy always a good metric?,"Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, a constant prediction of A class would have an accuracy of 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.",easy
What is the confusion table? What are the cells in this table?,"Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.",easy
"What are precision, recall, and F1-score?",* Precision and recall are classification evaluation metrics:,easy
Precision-recall trade-off ‍,Tradeoff means increasing one parameter would lead to decreasing of other. Precision-recall tradeoff occur due to increasing one of the parameter(precision or recall) while keeping the model same.,medium
What is the ROC curve? When to use it? ‍,ROC stands for *Receiver Operating Characteristics*. The diagrammatic representation that shows the contrast between true positive rate vs false positive rate. It is used when we need to predict the probability of the binary outcome.,medium
What is AUC (AU ROC)? When to use it? ‍,"AUC stands for *Area Under the ROC Curve*. ROC is a probability curve and AUC represents degree or measure of separability. Its used when we need to value how much model is capable of distinguishing between classes. The value is between 0 and 1, the higher the better.",medium
How to interpret the AU ROC score? ‍,AUC score is the value of *Area Under the ROC Curve*.,medium
What is the PR (precision-recall) curve? ‍,A *precision*-*recall curve* (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. Precision-recall curves (PR curves) are recommended for highly skewed domains where ROC curves may provide an excessively optimistic view of the performance.,medium
What is the area under the PR curve? Is it a useful metric? ‍I,"The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the curve with a range of threshold values as a single score.",medium
In which cases AU PR is better than AU ROC? ‍,What is different however is that AU ROC looks at a true positive rate TPR and false positive rate FPR while AU PR looks at positive predictive value PPV and true positive rate TPR.,medium
What do we do with categorical variables? ‍,"Categorical variables must be encoded before they can be used as features to train a machine learning model. There are various encoding techniques, including:",medium
Why do we need one-hot encoding? ‍,"If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.",medium
What is curse of dimensionality? ‍,"The curse of dimensionality is an issue that arises when working with high-dimensional data. It is often said that the curse of dimensionality is one of the main problems with machine learning. The curse of dimensionality refers to the fact that, as the number of dimensions (features) in a data set increases, the number of data points required to accurately learn the relationships between those features increases exponentially.",medium
What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? ‍,Answer here,medium
What is regularization? Why do we need it?,Regularization is used to reduce overfitting in machine learning models. It helps the models to generalize well and make them robust to outliers and noise in the data.,easy
Which regularization techniques do you know? ‍,"There are mainly two types of regularization,",medium
What kind of regularization techniques are applicable to linear models? ‍,"AIC/BIC, Ridge regression, Lasso, Elastic Net, Basis pursuit denoising, Rudin–Osher–Fatemi model (TV), Potts model, RLAD,",medium
How does L2 regularization look like in a linear model? ‍,L2 regularization adds a penalty term to our cost function which is equal to the sum of squares of models coefficients multiplied by a lambda hyperparameter. This technique makes sure that the coefficients are close to zero and is widely used in cases when we have a lot of features that might correlate with each other.,medium
How do we select the right regularization parameters?,"Regularization parameters can be chosen using a grid search, for example https://scikit-learn.org/stable/modules/linear_model.html has one formula for the implementing for regularization, alpha in the formula mentioned can be found by doing a RandomSearch or a GridSearch on a set of values and selecting the alpha which gives the least cross validation or validation error.",easy
What’s the effect of L2 regularization on the weights of a linear model? ‍,"L2 regularization penalizes larger weights more severely (due to the squared penalty term), which encourages weight values to decay toward zero.",medium
How L1 regularization looks like in a linear model? ‍,"L1 regularization adds a penalty term to our cost function which is equal to the sum of modules of models coefficients multiplied by a lambda hyperparameter. For example, cost function with L1 regularization will look like: ",medium
What’s the difference between L2 and L1 regularization? ‍,"- Penalty terms: L1 regularization uses the sum of the absolute values of the weights, while L2 regularization uses the sum of the weights squared.",medium
Can we have both L1 and L2 regularization components in a linear model? ‍,"Yes, elastic net regularization combines L1 and L2 regularization.",medium
What’s the interpretation of the bias term in linear models? ‍,"Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.",medium
How do we interpret weights in linear models? ‍,"Without normalizing weights or variables, if you increase the corresponding predictor by one unit, the coefficient represents on average how much the output changes. By the way, this interpretation still works for logistic regression - if you increase the corresponding predictor by one unit, the weight represents the change in the log of the odds.",medium
If a weight for one variable is higher than for another — can we say that this variable is more important? ‍,Yes - if your predictor variables are normalized.,medium
When do we need to perform feature normalization for linear models? When it’s okay not to do it? ‍,Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This cant be done effectively if every feature is scaled differently.,medium
Is feature selection important for linear models? ‍,"Yes, It is. It can make model performance better through selecting the most importance features and remove irrelevant features in order to make a prediction and it can also avoid overfitting, underfitting and bias-variance tradeoff.",medium
Which feature selection techniques do you know? ‍,Here are some of the feature selections:,medium
Can we use L1 regularization for feature selection? ‍,"Yes, because the nature of L1 regularization will lead to sparse coefficients of features. Feature selection can be done by keeping only features with non-zero coefficients.",medium
Can we use L2 regularization for feature selection? ‍,"No, Because L2 regularization does not make the weights zero but only makes them very very small. L2 regularization can be used to solve multicollinearity since it stabilizes the model.",medium
How do we train decision trees? ‍,1. Start at the root node.,medium
What are the main parameters of the decision tree model?,* maximum tree depth,easy
How do we handle categorical variables in decision trees? ‍,"Some decision tree algorithms can handle categorical variables out of the box, others cannot. However, we can transform categorical variables, e.g. with a binary or a one-hot encoder.",medium
What are the benefits of a single decision tree compared to more complex models? ‍,* easy to implement,medium
How can we know which features are more important for the decision tree model? ‍,"Often, we want to find a split such that it minimizes the sum of the node impurities. The impurity criterion is a parameter of decision trees. Popular methods to measure the impurity are the Gini impurity and the entropy describing the information gain.",medium
Why do we need randomization in random forest? ‍,"Random forest in an extension of the **bagging** algorithm which takes *random data samples from the training dataset* (with replacement), trains several models and averages predictions. In addition to that, each time a split in a tree is considered, random forest takes a *random sample of m features from full set of n features* (with replacement) and uses this subset of features as candidates for the split (for example, `m = sqrt(n)`).",medium
What are the main parameters of the random forest model? ‍,- `max_depth`: Longest Path between root node and the leaf,medium
How do we select the depth of the trees in random forest? ‍,"The greater the depth, the greater amount of information is extracted from the tree, however, there is a limit to this, and the algorithm even if defensive against overfitting may learn complex features of noise present in data and as a result, may overfit on noise. Hence, there is no hard thumb rule in deciding the depth, but literature suggests a few tips on tuning the depth of the tree to prevent overfitting:",medium
How do we know how many trees we need in random forest? ‍,"The number of trees in random forest is worked by n_estimators, and a random forest reduces overfitting by increasing the number of trees. There is no fixed thumb rule to decide the number of trees in a random forest, it is rather fine tuned with the data, typically starting off by taking the square of the number of features (n) present in the data followed by tuning until we get the optimal results.",medium
Is it easy to parallelize training of a random forest model? How can we do it? ‍,"Yes, R provides a simple way to parallelize training of random forests on large scale data.",medium
What are the potential problems with many large trees? ‍,"- Overfitting: A large number of large trees can lead to overfitting, where the model becomes too complex and is able to memorize the training data but doesnt generalize well to new, unseen data.",medium
"What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work?",Answer here,expert
What happens when we have correlated features in our data? ‍,"In random forest, since random forest samples some features to build each tree, the information contained in correlated features is twice as much likely to be picked than any other information contained in other features.",medium
What’s the difference between random forest and gradient boosting? ‍,1. Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.,medium
Is it possible to parallelize training of a gradient boosting model? How to do it? ‍,"Yes, different frameworks provide different options to make training faster, using GPUs to speed up the process by making it highly parallelizable.For example, for XGBoost tree_method = gpu_hist option makes training faster by use of GPUs.",medium
Feature importance in gradient boosting trees — what are possible options? ‍,Answer here,medium
Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models?,Answer here,expert
What are the main parameters in the gradient boosting model? ‍,"There are many parameters, but below are a few key defaults.",medium
How do you approach tuning parameters in XGBoost or LightGBM?,"Depending upon the dataset, parameter tuning can be done manually or using hyperparameter optimization frameworks such as optuna and hyperopt. In manual parameter tuning, we need to be aware of max-depth, min_samples_leaf and min_samples_split so that our model does not overfit the data but try to predict generalized characteristics of data (basically keeping variance and bias low for our model).",expert
How do you select the number of trees in the gradient boosting model? ‍,"Most implementations of gradient boosting are configured by default with a relatively small number of trees, such as hundreds or thousands. Using scikit-learn we can perform a grid search of the n_estimators model parameter",medium
What’s the difference between grid search parameter tuning strategy and random search? When to use one or another? ‍,"For specifics, refer to the above answer.",medium
How does a usual fully-connected feed-forward neural network work? ‍,"In a usual fully-connected feed-forward network, each neuron receives input from every element of the previous layer and thus the receptive field of a neuron is the entire previous layer. They are usually used to represent feature vectors for input data in classification problems but can be expensive to train because of the number of computations involved.",medium
Why do we need activation functions?,"The main idea of using neural networks is to learn complex nonlinear functions. If we are not using an activation function in between different layers of a neural network, we are just stacking up multiple linear layers one on top of another and this leads to learning a linear function. The Nonlinearity comes only with the activation function, this is the reason we need activation functions.",easy
What are the problems with sigmoid as an activation function? ‍,The derivative of the sigmoid function for large positive or negative numbers is almost zero. From this comes the problem of vanishing gradient — during the backpropagation our net will not learn (or will learn drastically slow). One possible way to solve this problem is to use ReLU activation function.,medium
What is ReLU? How is it better than sigmoid or tanh? ‍,"ReLU is an abbreviation for Rectified Linear Unit. It is an activation function which has the value 0 for all negative values and the value f(x) = x for all positive values. The ReLU has a simple activation function which makes it fast to compute and while the sigmoid and tanh activation functions saturate at higher values, the ReLU has a potentially infinite activation, which addresses the problem of vanishing gradients.",medium
How we can initialize the weights of a neural network? ‍,Proper initialization of weight matrix in neural network is very necessary.,medium
What if we set all the weights of a neural network to 0? ‍,"If all the weights of a neural network are set to zero, the output of each connection is same (W*x = 0). This means the gradients which are backpropagated to each connection in a layer is same. This means all the connections/weights learn the same thing, and the model never converges.",medium
What regularization techniques for neural nets do you know? ‍,"* L1 Regularization - Defined as the sum of absolute values of the individual parameters. The L1 penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded.",medium
What is dropout? Why is it useful? How does it work? ‍,"Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.",medium
Which optimization techniques for training neural nets do you know? ‍,* Gradient Descent,medium
How do we use SGD (stochastic gradient descent) for training a neural net? ‍,"SGD approximates the expectation with few randomly selected samples (instead of the full data). In comparison to batch gradient descent, we can efficiently approximate the expectation in large data sets using SGD. For neural networks this reduces the training time a lot even considering that it will converge later as the random sampling adds noise to the gradient descent.",medium
What’s the learning rate?,"The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the step width during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.",easy
What happens when the learning rate is too large? Too small?,"A large learning rate can accelerate the training. However, it is possible that we shoot too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights wont update even if it is not the best possible global solution.",easy
How to set the learning rate? ‍,There is no straightforward way of finding an optimum learning rate for a model. It involves a lot of hit and trial. Usually starting with a small values such as 0.01 is a good starting point for setting a learning rate and further tweaking it so that it doesnt overshoot or converge too slowly.,medium
What is Adam? What’s the main difference between Adam and SGD? ‍,"Adam (Adaptive Moment Estimation) is a optimization technique for training neural networks. on an average, it is the best optimizer .It works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.",medium
When would you use Adam and when SGD? ‍,"Adam tends to converge faster, while SGD often converges to more optimal solutions.",medium
Do we want to have a constant learning rate or we better change it throughout training? ‍,"Generally, it is recommended to start learning rate with relatively high value and then gradually decrease learning rate so the model does not overshoot the minima and at the same time we dont want to start with very low learning rate as the model will take too long to converge. There are many available techniques to do decay the learning rate. For example, in PyTorch you can use",medium
How do we decide when to stop training a neural net?,Simply stop training when the validation error is the minimum.,easy
What is model checkpointing? ‍,Saving the weights learned by a model mid training for long running processes is known as model checkpointing so that you can resume your training from a certain checkpoint.,medium
Can you tell us how you approach the model training process? ‍,Answer here,medium
What’s a convolutional layer? ‍,"The idea of the convolutional layer is the assumption that the information needed for making a decision often is spatially close and thus, it only takes the weighted sum over nearby inputs. It also assumes that the networks’ kernels can be reused for all nodes, hence the number of weights can be drastically reduced. To counteract only one feature being learnt per layer, multiple kernels are applied to the input which creates parallel channels in the output. Consecutive layers can also be stacked to allow the network to find more high-level features.",medium
Why do we actually need convolutions? Can’t we use fully-connected layers for that? ‍,"A fully-connected layer needs one weight per inter-layer connection, which means the number of weights which needs to be computed quickly balloons as the number of layers and nodes per layer is increased.",medium
What’s pooling in CNN? Why do we need it? ‍,"Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.",medium
How does max pooling work? Are there other pooling techniques? ‍,"Max pooling is a technique where the maximum value of a receptive field is passed on in the next feature map. The most commonly used receptive field is 2 x 2 with a stride of 2, which means the feature map is downsampled from N x N to N/2 x N/2. Receptive fields larger than 3 x 3 are rarely employed as too much information is lost.",medium
Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated?,"CNNs are not resistant to rotation by design. However, we can make our models resistant by augmenting our datasets with different rotations of the raw data. The predictions of a CNN will change if an image is rotated and we did not augment our dataset accordingly. A demonstration of this occurence can be seen in [this video](https://www.youtube.com/watch?v=VO1bQo4PXV4), where a CNN changes its predicted class between a duck and a rabbit based on the rotation of the image.",expert
What are augmentations? Why do we need them?,"Augmentations are an artifical way of expanding the existing datasets by performing some transformations, color shifts or many other things on the data. It helps in diversifying the data and even increasing the data when there is scarcity of data for a model to train on.",easy
What kind of augmentations do you know?,"There are many kinds of augmentations which can be used according to the type of data you are working on some of which are geometric and numerical transformation, PCA, cropping, padding, shifting, noise injection etc.",easy
How to choose which augmentations to use? ‍,"Augmentations really depend on the type of output classes and the features you want your model to learn. For eg. if you have mostly properly illuminated images in your dataset and want your model to predict poorly illuminated images too, you can apply channel shifting on your data and include the resultant images in your dataset for better results.",medium
What kind of CNN architectures for classification do you know?,Image Classification,expert
What is transfer learning? How does it work? ‍,"Given a source domain D_S and learning task T_S, a target domain D_T and learning task T_T, transfer learning aims to help improve the learning of the target predictive function f_T in D_T using the knowledge in D_S and T_S, where D_S ≠ D_T,or T_S ≠ T_T. In other words, transfer learning enables to reuse knowledge coming from other domains or learning tasks.",medium
What is object detection? Do you know any architectures for that?,Object detection is finding Bounding Boxes around objects in an image.,expert
What is object segmentation? Do you know any architectures for that?,Object Segmentation is predicting masks. It does not differentiate objects.,expert
What is bag of words? How we can use it for text classification? ‍,"Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.",medium
What are the advantages and disadvantages of bag of words? ‍,Advantages:,medium
What are N-grams? How can we use them? ‍,The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.,medium
How large should be N for our bag of words when using N-grams? ‍,Answer here,medium
What is TF-IDF? How is it useful for text classification? ‍,"Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highly recurring words may not contain as much informational content as the domain specific words. For example, words like the that are frequent across all documents therefore need to be less weighted. The TF-IDF score highlights words that are distinct (contain useful information) in a given document.",medium
Which model would you use for text classification with bag of words features? ‍,1. Bag Of Words model,medium
Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? ‍,Usually logistic regression is better because bag of words creates a matrix with large number of columns. For a huge number of columns logistic regression is usually faster than gradient boosting trees.,medium
What are word embeddings? Why are they useful? Do you know Word2Vec? ‍,"Word Embeddings are vector representations for words. Each word is mapped to one vector, this vector tries to capture some characteristics of the word, allowing similar words to have similar vector representations.",medium
Do you know any other ways to get word embeddings?,- TF-IDF,expert
"If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? ‍",Approaches ranked from simple to more complex:,medium
Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? ‍,Answer here,medium
How can you use neural nets for text classification?,Answer here,expert
How can we use CNN for text classification?,Answer here,expert
What is clustering? When do we need it?,Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.,easy
Do you know how K-means works? ‍,1. Partition points into k subsets.,medium
How to select K for K-means? ‍,"* Domain knowledge, i.e. an expert knows the value of k",medium
What are the other clustering algorithms do you know? ‍,* k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.,medium
Do you know how DBScan works? ‍,* Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood),medium
When would you choose K-means and when DBScan? ‍,* DBScan is more robust to noise.,medium
Do you know any dimensionality reduction techniques? ‍,* Singular Value Decomposition (SVD),medium
What’s singular value decomposition? How is it typically used for machine learning? ‍,"* Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Σ (diagonal matrix) and R^T (right singular values).",medium
What are good unsupervised baselines for text information retrieval? ‍,Answer here,medium
How would you evaluate your ranking algorithms? Which offline metrics would you use? ‍,Answer here,medium
What is precision and recall at k? ‍,Precision at k and recall at k are evaluation metrics for ranking algorithms. Precision at k shows the share of relevant items in the first *k* results of the ranking algorithm. And Recall at k indicates the share of relevant items returned in top *k* results out of all correct answers for a given query.,medium
What is mean average precision at k? ‍,Answer here,medium
How can we use machine learning for search? ‍,Answer here,medium
How can we get training data for our ranking algorithms? ‍,Answer here,medium
Can we formulate the search problem as a classification problem? How? ‍,Answer here,medium
How can we use clicks data as the training data for ranking algorithms?,Answer here,expert
Do you know how to use gradient boosting trees for ranking?,Answer here,expert
How do you do an online evaluation of a new ranking algorithm? ‍,Answer here,medium
What are good baselines when building a recommender system? ‍,* A good recommer system should give relevant and personalized information.,medium
What is collaborative filtering? ‍,* Collaborative filtering is the most prominent approach to generate recommendations.,medium
"How we can incorporate implicit feedback (clicks, etc) into our recommender systems? ‍","In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.",medium
What is the cold start problem? ‍,"Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.",medium
Possible approaches to solving the cold start problem? ‍,"* Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependent on the ratings of other users for a given item anymore and solve the cold start problem for new items.",medium
How is time series different from the usual regression problem?,"The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.",easy
Which models do you know for solving time series problems? ‍,* Simple Exponential Smoothing: approximate the time series with an exponential function,medium
"If there’s a trend in our series, how we can remove it? And why would we want to do it? ‍","We can explicitly model the trend (and/or seasonality) with approaches such as Holts Method or Holt-Winters Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].",medium
You have a series with only one variable y measured at time t. How do predict y at time t+1? Which approaches would you use? ‍,We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.,medium
You have a series with a variable y and a set of features. How do you predict y at t+1? Which approaches would you use? ‍,"Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explainability of the results is not a high priority, we can also consider deep learning approaches.",medium
What are the problems with using trees for solving time series problems? ‍,Random Forest models are not able to extrapolate time series data and understand increasing/decreasing trends. It will provide us with average data points if the validation data has values greater than the training data points.,medium
